{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAEchn3OT0T9tJTpkRihl1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YahiaML/Linkedin-Web-scraping-series/blob/main/8_Scrap_in_structured_format.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xDDwiFcNq1hC"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lets test how to scrap the Job Details and the Preferred Candidate info in a clean format"
      ],
      "metadata": {
        "id": "ib-WoSX5yiH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How we were Job Details scraping them before\n",
        "url = 'https://www.bayt.com/en/egypt/jobs/executive-secretary-to-the-chairman-5175578/'  # random page to test on\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "\n",
        "# Job Details\n",
        "try:\n",
        "    job_details = soup.find(\"dl\", {\"class\": \"dlist is-spaced is-fitted t-small\"}).text.strip()\n",
        "except:\n",
        "    job_details = np.nan\n",
        "\n",
        "print(job_details)\n"
      ],
      "metadata": {
        "id": "wwgUmAUL3xLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How we were Preferred Candidate scraping them before\n",
        "try:\n",
        "    preferred_candidate = [value.text.strip() for value in soup.find(\"h2\", {\"class\": \"h5\"}, string=\"Preferred Candidate\").find_next_siblings(\"dl\")]\n",
        "except:\n",
        "    preferred_candidate = np.nan\n",
        "\n",
        "print(preferred_candidate)"
      ],
      "metadata": {
        "id": "v2PXWE7H4A8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This is not the most neat way to do that. So, lets try another method"
      ],
      "metadata": {
        "id": "7f993PBa4Y7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://www.bayt.com/en/egypt/jobs/executive-secretary-to-the-chairman-5175578/'  # random page to test on\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "try:\n",
        "    job_details = {}\n",
        "    job_details_list = [value for value in soup.find(\"h2\", {\"class\": \"h5\"}, string=\"Job Details\").find_next_sibling(\"dl\").find_all(\"div\")]\n",
        "    for detail in job_details_list:\n",
        "        job_details[detail.find(\"dt\").text] = detail.find(\"dd\").text\n",
        "except:\n",
        "    job_details = np.nan\n",
        "job_details"
      ],
      "metadata": {
        "id": "gBW2Dld_sm8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    preferred_candidate = {}\n",
        "    preferred_candidate_requirements = [value for value in soup.find(\"h2\", {\"class\": \"h5\"}, string=\"Preferred Candidate\").find_next_siblings(\"dl\")]\n",
        "    for requirement in preferred_candidate_requirements:\n",
        "        preferred_candidate[requirement.find(\"dt\").text] = requirement.find(\"dd\").text\n",
        "except:\n",
        "    preferred_candidate = np.nan\n",
        "preferred_candidate"
      ],
      "metadata": {
        "id": "M3qbS2NNq2Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now, lets combine those to the main script"
      ],
      "metadata": {
        "id": "fA7c4krfyw8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0:  Determine the number of pages"
      ],
      "metadata": {
        "id": "dOHGNsM91CZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the number of pages\n",
        "url = 'https://www.bayt.com/en/egypt/jobs/data-analysis-jobs'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "number_of_jobs = int(soup.find(\"b\", {\"data-automation-id\": \"XJobsFound\"}).text.replace(\"jobs found\", \"\").strip())\n",
        "number_of_pages = int(np.ceil(number_of_jobs / 20))"
      ],
      "metadata": {
        "id": "vNhI7BRMz0Du"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Scrap all main pages and sub-pages links"
      ],
      "metadata": {
        "id": "5sYxlXm-U1WB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Lists to store data\n",
        "job_titles, job_links, company_names, locations, posted_from_list, job_types, experience_list, other_info_list = [],[],[],[],[],[],[],[]\n",
        "\n",
        "# Step 1: Determine URL pattern\n",
        "for page_number in range(1,number_of_pages+1):\n",
        "    url = f'https://www.bayt.com/en/egypt/jobs/data-analysis-jobs/?page={page_number}'\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Step 2: Use find_all to get the containers that hold all job info\n",
        "    job_containers = soup.find_all('li', {'class': 'has-pointer-d'})\n",
        "\n",
        "    # Step 3: Iterate over each container and extract relevant data\n",
        "    for container in job_containers:\n",
        "\n",
        "        # Job title\n",
        "        try:\n",
        "            job_title = container.find('h2').text.strip()\n",
        "        except:\n",
        "            job_title = np.nan\n",
        "\n",
        "        # Job link\n",
        "        try:\n",
        "            job_link = container.find('h2').find('a').get('href')\n",
        "            job_link = \"https://www.bayt.com\" + job_link  # Making it a full URL\n",
        "        except:\n",
        "            job_link = np.nan\n",
        "\n",
        "        # Company name\n",
        "        try:\n",
        "            company_name = container.find('b').text.strip()\n",
        "        except:\n",
        "            company_name = np.nan\n",
        "\n",
        "        # Location\n",
        "        try:\n",
        "            location = container.find('div', {'class': 't-mute'}).text.strip()\n",
        "        except:\n",
        "            location = np.nan\n",
        "\n",
        "        # Posted from\n",
        "        try:\n",
        "            posted_from = container.find('div', {'data-automation-id': 'job-active-date'}).text.strip()\n",
        "        except:\n",
        "            posted_from = np.nan\n",
        "\n",
        "        # Job type (Remote/On-site)\n",
        "        try:\n",
        "            job_type = container.find('li', {'class': 'jb-label-remote'}).text.strip()\n",
        "        except:\n",
        "            job_type = np.nan\n",
        "\n",
        "        # Experience level and years of experience\n",
        "        try:\n",
        "            experience = container.find('li', {'class': 'jb-label-careerlevel'}).text.strip()\n",
        "        except:\n",
        "            experience = np.nan\n",
        "\n",
        "        # Additional info (if any)\n",
        "        try:\n",
        "            other_info = container.find('div', {'class': 'm10t t-small'}).text.strip()\n",
        "        except:\n",
        "            other_info = np.nan\n",
        "\n",
        "        # Append info to relevant lists\n",
        "        job_titles.append(job_title)\n",
        "        job_links.append(job_link)\n",
        "        company_names.append(company_name)\n",
        "        locations.append(location)\n",
        "        posted_from_list.append(posted_from)\n",
        "        job_types.append(job_type)\n",
        "        experience_list.append(experience)\n",
        "        other_info_list.append(other_info)\n",
        "\n",
        "# Create a DataFrame\n",
        "jobs_df = pd.DataFrame({'Job Title': job_titles,'Job Link': job_links,'Company Name': company_names,'Location': locations,'Posted From': posted_from_list,'Job Type': job_types,'Experience': experience_list,\"Additional Info\": other_info_list})\n",
        "\n",
        "jobs_df\n"
      ],
      "metadata": {
        "id": "zBk63Ul90y5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Scrap the whole website data"
      ],
      "metadata": {
        "id": "Ij9yc56FU_6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Lists to store the data\n",
        "job_titles, companies, company_pages, company_locations, posting_dates, will_be_closed_ats, salary_ranges, full_descriptions, full_description_htmls, skills_list, skills_htmls, jobs_details_list, job_details_htmls, preferred_candidates, preferred_candidates_htmls, = [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
        "\n",
        "for index,row in jobs_df.iterrows():\n",
        "    url = row['Job Link']\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "\n",
        "    # Job Title\n",
        "    try:\n",
        "        job_title = soup.find(\"h1\").text.strip()\n",
        "    except:\n",
        "        job_title = np.nan\n",
        "\n",
        "    # Company\n",
        "    try:\n",
        "        company = soup.find(\"a\", {\"class\": \"is-black t-bold\"}).text.strip()\n",
        "    except:\n",
        "        company = np.nan\n",
        "\n",
        "    # Company Page\n",
        "    try:\n",
        "        company_page = \"https://www.bayt.com\" + soup.find(\"a\", {\"class\": \"is-black t-bold\"}).get(\"href\")\n",
        "    except:\n",
        "        company_page = np.nan\n",
        "\n",
        "    # Company Location\n",
        "    try:\n",
        "        company_location = soup.find(\"span\", {\"class\": \"t-mute\"}).text.strip()\n",
        "    except:\n",
        "        company_location = np.nan\n",
        "\n",
        "    # Posting Date\n",
        "    try:\n",
        "        posting_date = soup.find(\"div\", {\"class\": \"t-small\"}).find(\"span\").text.strip()\n",
        "    except:\n",
        "        posting_date = np.nan\n",
        "\n",
        "    # Will Be Closed At\n",
        "    try:\n",
        "        will_be_closed_at = soup.find(\"div\", {\"class\": \"t-small\"}).find(\"span\", {\"class\": \"u-none\"}).text.strip()\n",
        "    except:\n",
        "        will_be_closed_at = np.nan\n",
        "\n",
        "    # Salary Range\n",
        "    try:\n",
        "        salary_range = soup.find(\"b\", {\"class\": \"t-small\"}).text.strip()\n",
        "    except:\n",
        "        salary_range = np.nan\n",
        "\n",
        "    # Full Description\n",
        "    try:\n",
        "        full_description = soup.find(\"div\", {\"class\": \"card-content p20t is-spaced\"}).find(\"div\", {\"class\": \"t-break\"}).text.strip()\n",
        "    except:\n",
        "        full_description = np.nan\n",
        "\n",
        "    # Full Description HTML\n",
        "    try:\n",
        "        full_description_html = soup.find(\"div\", {\"class\": \"card-content p20t is-spaced\"}).find(\"div\", {\"class\": \"t-break\"})\n",
        "    except:\n",
        "        full_description_html = np.nan\n",
        "\n",
        "    # Skills\n",
        "    try:\n",
        "        skills = soup.find(\"div\", {\"class\": \"card-content is-spaced t-break print-break-before p20t\"}).text.strip()\n",
        "    except:\n",
        "        skills = np.nan\n",
        "\n",
        "    # Skills HTML\n",
        "    try:\n",
        "        skills_html = soup.find(\"div\", {\"class\": \"card-content is-spaced t-break print-break-before p20t\"})\n",
        "    except:\n",
        "        skills_html = np.nan\n",
        "\n",
        "    # Job Details\n",
        "    try:\n",
        "        job_details = {}\n",
        "        job_details_list = [value for value in soup.find(\"h2\", {\"class\": \"h5\"}, string=\"Job Details\").find_next_sibling(\"dl\").find_all(\"div\")]\n",
        "        for detail in job_details_list:\n",
        "            job_details[detail.find(\"dt\").text] = detail.find(\"dd\").text\n",
        "    except:\n",
        "        job_details = np.nan\n",
        "\n",
        "    # Preferred Candidate\n",
        "    try:\n",
        "        preferred_candidate = {}\n",
        "        preferred_candidate_requirements = [value for value in soup.find(\"h2\", {\"class\": \"h5\"}, string=\"Preferred Candidate\").find_next_siblings(\"dl\")]\n",
        "        for requirement in preferred_candidate_requirements:\n",
        "            preferred_candidate[requirement.find(\"dt\").text] = requirement.find(\"dd\").text\n",
        "    except:\n",
        "        preferred_candidate = np.nan\n",
        "\n",
        "\n",
        "    # append\n",
        "    job_titles.append(job_title)\n",
        "    companies.append(company)\n",
        "    company_pages.append(company_page)\n",
        "    company_locations.append(company_location)\n",
        "    posting_dates.append(posting_date)\n",
        "    will_be_closed_ats.append(will_be_closed_at)\n",
        "    salary_ranges.append(salary_range)\n",
        "    full_descriptions.append(full_description)\n",
        "    full_description_htmls.append(full_description_html)\n",
        "    skills_list.append(skills)\n",
        "    skills_htmls.append(skills_html)\n",
        "    jobs_details_list.append(job_details)\n",
        "    preferred_candidates.append(preferred_candidate)\n",
        "\n",
        "# Create DataFrame\n",
        "full_df = pd.DataFrame({\"Job Title\": job_titles,\"Company\": companies,\"Company Page\": company_pages,\"Company Location\": company_locations,\"Posting Date\": posting_dates,\"Will Be Closed At\": will_be_closed_ats,\"Salary Range\": salary_ranges,\"Full Description\": full_descriptions,\"Full Description HTML\": full_description_htmls,\"Skills\": skills_list,\"Skills HTML\": skills_htmls,\"Job Details\": jobs_details_list,\"Preferred Candidate\": preferred_candidates})\n",
        "\n",
        "# Print the DataFrame\n",
        "full_df"
      ],
      "metadata": {
        "id": "yn_dkQDl05jP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}