{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSPCSRhQLp7cP3XJjRI2We",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YahiaML/Linkedin-Web-scraping-series/blob/main/3_Info_containers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Get the webpage HTML\n",
        "url = 'https://www.bayt.com/en/egypt/jobs/data-analysis-jobs/'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Step 2: Use find_all to get the containers that hold all job info\n",
        "job_containers = soup.find_all('li', {'class': 'has-pointer-d'})\n",
        "\n",
        "# Lists to store data\n",
        "job_titles, job_links, company_names, locations, posted_from_list, job_types, experience_list, other_info_list = [],[],[],[],[],[],[],[]\n",
        "\n",
        "# Step 3: Iterate over each container and extract relevant data\n",
        "for container in job_containers:\n",
        "\n",
        "    # Job title\n",
        "    try:\n",
        "        job_title = container.find('h2').text.strip()\n",
        "    except:\n",
        "        job_title = np.nan\n",
        "\n",
        "    # Job link\n",
        "    try:\n",
        "        job_link = container.find('h2').find('a').get('href')\n",
        "        job_link = \"https://www.bayt.com\" + job_link  # Making it a full URL\n",
        "    except:\n",
        "        job_link = np.nan\n",
        "\n",
        "    # Company name\n",
        "    try:\n",
        "        company_name = container.find('b').text.strip()\n",
        "    except:\n",
        "        company_name = np.nan\n",
        "\n",
        "    # Location\n",
        "    try:\n",
        "        location = container.find('div', {'class': 't-mute'}).text.strip()\n",
        "    except:\n",
        "        location = np.nan\n",
        "\n",
        "    # Posted from\n",
        "    try:\n",
        "        posted_from = container.find('div', {'data-automation-id': 'job-active-date'}).text.strip()\n",
        "    except:\n",
        "        posted_from = np.nan\n",
        "\n",
        "    # Job type (Remote/On-site)\n",
        "    try:\n",
        "        job_type = container.find('li', {'class': 'jb-label-remote'}).text.strip()\n",
        "    except:\n",
        "        job_type = np.nan\n",
        "\n",
        "    # Experience level and years of experience\n",
        "    try:\n",
        "        experience = container.find('li', {'class': 'jb-label-careerlevel'}).text.strip()\n",
        "    except:\n",
        "        experience = np.nan\n",
        "\n",
        "    # Additional info (if any)\n",
        "    try:\n",
        "        other_info = container.find('div', {'class': 'm10t t-small'}).text.strip()\n",
        "    except:\n",
        "        other_info = np.nan\n",
        "\n",
        "    # Append info to relevant lists\n",
        "    job_titles.append(job_title)\n",
        "    job_links.append(job_link)\n",
        "    company_names.append(company_name)\n",
        "    locations.append(location)\n",
        "    posted_from_list.append(posted_from)\n",
        "    job_types.append(job_type)\n",
        "    experience_list.append(experience)\n",
        "    other_info_list.append(other_info)\n",
        "\n",
        "# Create a DataFrame\n",
        "jobs_df = pd.DataFrame({\n",
        "    'Job Title': job_titles,\n",
        "    'Job Link': job_links,\n",
        "    'Company Name': company_names,\n",
        "    'Location': locations,\n",
        "    'Posted From': posted_from_list,\n",
        "    'Job Type': job_types,\n",
        "    'Experience': experience_list,\n",
        "    \"Additional Info\": other_info_list\n",
        "})\n",
        "\n",
        "jobs_df\n"
      ],
      "metadata": {
        "id": "6IpZ2ZxiWaZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === What will happen if you only relayed on find_all() and didn't follow the 2 metioned steps == #\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Get the webpage HTML\n",
        "url = 'https://www.bayt.com/en/egypt/jobs/data-analysis-jobs/'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# use find_all() for direct extraction of each field without handling missing data\n",
        "job_titles = [job.text.strip() for job in soup.find_all('h2')]\n",
        "job_links = [\"https://www.bayt.com\" + job.find('a').get('href') for job in soup.find_all('h2')]\n",
        "company_names = [company.text.strip() for company in soup.find_all('b')]\n",
        "locations = [location.text.strip() for location in soup.find_all('div', {'class': 't-mute'})]\n",
        "posted_from_list = [posted.text.strip() for posted in soup.find_all('div', {'data-automation-id': 'job-active-date'})]\n",
        "job_types = [job_type.text.strip() for job_type in soup.find_all('li', {'class': 'jb-label-remote'})]\n",
        "experience_list = [experience.text.strip() for experience in soup.find_all('li', {'class': 'jb-label-careerlevel'})]\n",
        "other_info_list = [other_info.text.strip() for other_info in soup.find_all('div', {'class': 'm10t t-small'})]\n",
        "\n",
        "# Attempt to create a DataFrame\n",
        "try:\n",
        "    jobs_df = pd.DataFrame({\n",
        "      'Job Title': job_titles,\n",
        "      'Job Link': job_links,\n",
        "      'Company Name': company_names,\n",
        "      'Location': locations,\n",
        "      'Posted From': posted_from_list,\n",
        "      'Job Type': job_types,\n",
        "      'Experience': experience_list,\n",
        "      \"Additional Info\": other_info_list\n",
        "      })\n",
        "\n",
        "    jobs_df\n",
        "except Exception as e:\n",
        "  print(\"Error creating DataFrame:\")\n",
        "  print(e)\n",
        "  print(\"-------------------------------\")\n",
        "# This is caused by\n",
        "print(\"number of collected job_titles:\",len(job_titles))\n",
        "print(\"number of collected job_links:\",len(job_links))\n",
        "print(\"number of collected company_names:\",len(company_names))\n",
        "print(\"number of collected locations:\",len(locations))\n",
        "print(\"number of collected posted_from_list:\",len(posted_from_list))\n",
        "print(\"number of collected job_types:\",len(job_types))\n",
        "print(\"number of collected experience_list:\",len(experience_list))\n",
        "print(\"number of collected other_info_list:\",len(other_info_list))\n"
      ],
      "metadata": {
        "id": "a4-aw8x6ixoP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}